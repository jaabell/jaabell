Title: CS261 - HW # 0 : Describe a parallel application
Slug: cs261-hw-0-describe-a-parallel-application
Date: 2015-01-29 16:57:28
Tags: cs261, parallel computing, 
Category: CS261 Homeworks
Author: Jose Abell
Lang: en
Summary: This is homework # 1 for UC Berkeley class CS261: Application of Parallel Computers
Status: draft

by: José Abell

Biography
==========

I am a PhD student at UCDavis [CompGeoMech][CompGeoMech] since Sept 2011, working on our in-house high-performance FEM simulation system. I come from Chile, where I did my undergraduate studies in structural engineering, leading to a professional degree in the specialty, at Pontificia Universidad Católica de Chile. Then, I proceeded to do my MS at the same institution in Civil Engineering. My MS thesis was more focused into the hazard part of earthquake engineering. 

My current research focuses on high-performance Earthquake-Soil-Structure interaction (ESSI) simulation. In a nutshell, it consists in modeling the effect of earthquake on structures from the earthquake source rupture process all the way up to a structure and its contents. The idea is to find out how and when a more accurate ESSI simulation yields different results from what is currently done in practice, and if this leads to safer and more economical designs. 

Parallel, high-performance computing is an enabling technology in this endeavor and there are many opportunities along the process where leveraging parallelism is possible. Meshing, simulation, and post-processing are important examples of these. 

Out of CS261 is would like to get insight into what it takes to get extreme-performance out of simulation software in different architectures. In particular, I would like to know more about the following topics:

* MPI usage patterns, how they come up, when to apply them, and tradeoffs.
* Shared memory parallel programming models (pthreads, OpenMP) and recurring software patterns. 
* Parallel I/O techniques. 



Application: ESSI Simulator
=============================

Introduction
#############

The [Real ESSI Simulator][essi][[1](#essiref) and [2](#essiref2)]  is a system for simulation of ESSI problems developed at UC Davis. It consists of software, hardware (a parallel computer: ESSI simulator machine), and documentation covering theory, usage and examples for the system.

ESSI program is a parallel object-oriented finite element analysis (FEA) software for non-linear time domain analysis of ESSI systems. The program is written in C++, using several external libraries to accomplish its goals, most notably [OpenMPI][OpenMPI] (message passing interface) is used to achieve parallelism. Other libraries used within ESSI include: [PETSc][petsc] for parallel solution of system of equations, [METIS][metis] and [Par-METIS][parmetis] for graph partitioning, [HDF5]phdf5 for parallel output. Input is controlled by a custom domain-specific language designed specifically for this program. 

Parallelism in FEA
###################

![npp](/images/other/npp.png "Nuclear power plant model and its decomposition.")

Two main sources of parallelism can be identified in the context of nonlinear, dynamic finite element simulation: (i) system of equation solution and (ii) element-level constitutive integration. The first of these consists in the solution of a large linear system of equations (SOE) which arises from the discretization of the continuum problem (expressed as a set of coupled partial differential equations) in the spatial domain. The second source, comes from advancing the constitutive rate equations within each element once a global displacement increment is obtained from the solution of the SOE. This last part can account for a large part of the computational time for large problems and is embarrassingly parallel.

An additional source of parallelism in ESSI simulations is the storage of the large ammounts of output generated by these simulations. The philosophy adopted by the ESSI simulator is to independently store the information necessary to build the model and restart the simulation at any given point. This gives rise to possibly terabytes of data in even modest models, with the additional problem on how to handle this. In ESSI this is done by using a [network filesystem][nfs] (NFS) to create a virtual parallel unique disk and the HDF5 format to store the data. In a nutshell, HDF5 implements a format for storing scientific (array-oriented) data in a portable way, and also allowing parallel read/write (it uses MPI I/O under the hood).

A particularity of non-linear (plasticity based) FEA simulation is the unknown parts of the domain may plastify during simulations, leading to increased time spent integrating constitutive equations in that portion of the domain. What this implies is that, given an initial partition that balances the loading, this partition might become unbalanced if the domain plastifies. An adaptation of the dynamic domain decomposition method termed the "plastic domain decomposition"[[3](#pdd)], which achieves computational load re-balancing by repartitioning the element graph using computational time as one of the weighting factors.





References
===========

1. <a id="essiref"></a> Boris Jeremić, Robert Roche-Rivera, Annie Kammerer, Nima Tafazzoli, Jose Abell M., Babak Kamranimoghaddam, Federico Pisano, ChangGyun Jeong and Benjamin Aldridge The NRC ESSI Simulator Program, Current Status in Proceedings of the Structural Mechanics in Reactor Technology (SMiRT) 2013 Conference, San Francisco, August 18-23, 2013.

2. <a id="essiref2"></a> Boris  Jeremić, Guanzhou Jie, Matthias Preisig and Nima Tafazzoli. Time domain simulation of soil-foundation-structure interaction in non-uniform soils. Earthquake Engineering and Structural Dynamics, Volume 38, Issue 5, pp 699-718, 2009.


3. <a id="pdd"></a> Boris Jeremić and Guanzhou Jie. Plastic Domain Decomposition Method for Parallel Elastic–Plastic Finite Element Computations in Geomechanics Report UCD CompGeoMech 03–2007.







[CompGeoMech]: http://sokocalo.engr.ucdavis.edu/~jeremic/
[essi]: http://sokocalo.engr.ucdavis.edu/~jeremic/ESSI_Simulator/
[OpenMPI]: www.open-mpi.org/
[petsc]:  http://www.mcs.anl.gov/petsc/
[metis]: http://glaros.dtc.umn.edu/gkhome/metis/metis/overview
[parmetis]: http://glaros.dtc.umn.edu/gkhome/metis/parmetis/overview
[nfs]: http://en.wikipedia.org/wiki/Network_File_System